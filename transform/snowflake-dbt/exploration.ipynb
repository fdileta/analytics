{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow\n",
    "\n",
    "- A new version is released\n",
    "- We get a new version of sql-export json file\n",
    "- We check if we have all the source tables in the DWH\n",
    "- we create the models\n",
    "- we run the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imported libraries\n",
    "\n",
    "import json \n",
    "import sqlparse\n",
    "from sqlparse.sql import Identifier, IdentifierList, remove_quotes, Token, TokenList, Where\n",
    "from sqlparse.tokens import Keyword, Name, Punctuation, String, Whitespace\n",
    "from sqlparse.utils import imt\n",
    "import pandas as pd\n",
    "from flatten_dict import flatten\n",
    "from pprint import pprint\n",
    "import sql_metadata\n",
    "import re\n",
    "import os\n",
    "import urllib.request as request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Files imported\n",
    "\n",
    "## counter queries (sql+redis): this is generated manually by the product intelligence team\n",
    "## available here\n",
    "\n",
    "json_file_path = '/Users/mathieupeychet/Downloads/usage_ping_sql_jan.json'\n",
    "\n",
    "\n",
    "## foreign_key csv generated manually once by m_walker\n",
    "sql_query_to_generate = \"\"\"SELECT\n",
    "    tc.table_schema, \n",
    "    tc.constraint_name, \n",
    "    tc.table_name, \n",
    "    kcu.column_name, \n",
    "    ccu.table_schema AS foreign_table_schema,\n",
    "    ccu.table_name AS foreign_table_name,\n",
    "    ccu.column_name AS foreign_column_name \n",
    "FROM \n",
    "    information_schema.table_constraints AS tc \n",
    "    JOIN information_schema.key_column_usage AS kcu\n",
    "      ON tc.constraint_name = kcu.constraint_name\n",
    "      AND tc.table_schema = kcu.table_schema\n",
    "    JOIN information_schema.constraint_column_usage AS ccu\n",
    "      ON ccu.constraint_name = tc.constraint_name\n",
    "      AND ccu.table_schema = tc.table_schema\n",
    "WHERE tc.constraint_type = 'FOREIGN KEY' --AND tc.table_name='mytable'\n",
    "\"\"\"\n",
    "foreign_key_df = pd.read_csv('/Users/mathieupeychet/Documents/foreign_keys.csv')\n",
    "\n",
    "## To join an event to a ultimate_namespace_id, we have 3 potential standard tables to join\n",
    "table_to_join = ['projects', 'namespaces', 'groups']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SELECT COUNT(\"users\".\"id\") FROM \"users\" WHERE (\"users\".\"state\" IN (\\'active\\')) AND (\"users\".\"user_type\" IS NULL OR \"users\".\"user_type\" IN (NULL, 6, 4))'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sql_queries_dict(json_file):\n",
    "    ''' \n",
    "    function that transforms the sql-export.json file into a Python dict with only SQL batch counters\n",
    "    '''\n",
    "    with open(json_file) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    from flatten_dict.reducer import make_reducer\n",
    "    full_payload_dict = flatten(data, reducer=make_reducer(delimiter='.'))\n",
    "\n",
    "    sql_queries_dict  = {}\n",
    "\n",
    "    for (key, value) in full_payload_dict.items():\n",
    "       # Check if key is even then add pair to new dictionary\n",
    "       if isinstance(value, str) and str.startswith(value, 'SELECT') is True:\n",
    "           sql_queries_dict[key] = value\n",
    "    \n",
    "    return sql_queries_dict\n",
    "\n",
    "sql_queries_dict = sql_queries_dict(json_file_path)\n",
    "\n",
    "sql_queries_dict['active_user_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "### Small section that extracts all the needed tables for usage ping generation \n",
    "###\n",
    "needed_tables = []\n",
    "\n",
    "\n",
    "for key, value in sql_queries_dict.items():\n",
    "    sql_value = sqlparse.parse(value)[0]\n",
    "    ## get the table which is queried in the FROM statement\n",
    "    queried_tables_list = sql_metadata.get_query_tables(value)\n",
    "    for table in queried_tables_list:\n",
    "        if table not in needed_tables:\n",
    "            needed_tables.append(table)\n",
    "\n",
    "print(len(needed_tables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "def create_join_mapping_df(sql_queries_dict):\n",
    "    '''\n",
    "        The functoin returns a dataframe with the following columns\n",
    "        - counter: name of the counter, which is the item key in the dictionary passed\n",
    "        - sql_query: query run to calculate the counter, item value in the dictionary passed as argument\n",
    "        - table_name: name of the table in the FROM statement of the sql_query\n",
    "        - foreingn_table_name: \n",
    "        - foreign_column_name:\n",
    "    '''\n",
    "    final_join_mapping_df = pd.DataFrame()\n",
    "    for key, value in sql_queries_dict.items():\n",
    "        sql_value = sqlparse.parse(value)[0]\n",
    "        ## get the table which is queried in the FROM statement\n",
    "        queried_tables_list = sql_metadata.get_query_tables(value)\n",
    "        value = value.replace('\"', \"\")\n",
    "        \n",
    "        value= re.sub('{:start=>[0-9]{1,}, :finish=>[0-9]{1,}}', 'id', value)\n",
    "        # if projects is just do the join on projects\n",
    "        if 'projects' in queried_tables_list:\n",
    "            potential_joins = foreign_key_df[(foreign_key_df['table_name'] == 'projects') & (foreign_key_df['foreign_table_name'] == 'namespaces')]\n",
    "            potential_joins = potential_joins.drop_duplicates()\n",
    "            table_to_append = potential_joins[potential_joins.table_name == 'projects']\n",
    "            table_to_append[\"counter\"] = key\n",
    "            table_to_append[\"sql_query\"] = value\n",
    "            final_join_mapping_df = final_join_mapping_df.append(table_to_append, ignore_index=True)   \n",
    "        else:\n",
    "            for index, queried_table in enumerate(queried_tables_list):\n",
    "\n",
    "            ## \n",
    "                potential_joins = foreign_key_df[(foreign_key_df['table_name'] == queried_table) & (foreign_key_df['foreign_table_name'].isin(table_to_join))]\n",
    "                potential_joins = potential_joins.drop_duplicates()\n",
    "                \n",
    "                if potential_joins[potential_joins.foreign_table_name == 'projects'].empty is False:\n",
    "                    table_to_append = potential_joins[potential_joins.foreign_table_name == 'projects']\n",
    "                    table_to_append[\"counter\"] = key\n",
    "                    table_to_append[\"sql_query\"] = value\n",
    "                    final_join_mapping_df = final_join_mapping_df.append(table_to_append, ignore_index=True)\n",
    "                elif potential_joins[potential_joins.foreign_table_name == 'groups'].empty is False:\n",
    "                    table_to_append = potential_joins[potential_joins.foreign_table_name == 'groups']\n",
    "                    table_to_append[\"counter\"] = key\n",
    "                    table_to_append[\"sql_query\"] = value\n",
    "                    final_join_mapping_df = final_join_mapping_df.append(table_to_append, ignore_index=True)\n",
    "                elif potential_joins[potential_joins.foreign_table_name == 'namespaces'].empty is False:\n",
    "                    table_to_append = potential_joins[potential_joins.foreign_table_name == 'namespaces']\n",
    "                    table_to_append[\"counter\"] = key\n",
    "                    table_to_append[\"sql_query\"] = value\n",
    "                    final_join_mapping_df = final_join_mapping_df.append(table_to_append, ignore_index=True)\n",
    "                if table_to_append.empty is False:\n",
    "                    break\n",
    "                \n",
    "        \n",
    "    return final_join_mapping_df\n",
    "\n",
    "fj_df = create_join_mapping_df(sql_queries_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SQL Transformation\n",
    "\n",
    "This part of the code transforms the SQL query extracted directly from usage pings into SQL queries runable in the DWH\n",
    "\n",
    "The goal of these transformations is to get 2 different types of queries:\n",
    "\n",
    "##### Instance-level query for version 1 of Usage Ping\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "SELECT\n",
    "'count.issues'        AS counter_name,\n",
    "COUNT(issues.id)      AS counter_value,\n",
    "TO_DATE(CURRENT_DATE) AS run_day\n",
    "FROM prep.gitlab_dotcom.gitlab_dotcom_issues_dedupe_source\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_counter_name_as_column(counter_row):\n",
    "    '''\n",
    "    step needed to add the first 2 columns:\n",
    "      - counter_name\n",
    "      - run_day\n",
    "    \n",
    "    this needs a specific row of a specific dataframe, I think this could be changed to a SQL query for more convenience\n",
    "    \n",
    "    a query like that SELECT COUNT(issues.id) FROM issues will be changed to SELECT 'counts.issues', COUNT(issues.id), TO_DATE(CURRENT_DATE)\n",
    "    \n",
    "    needed for version 1 and 2\n",
    "    '''\n",
    "    \n",
    "    sql_query_parsed = sqlparse.parse(counter_row['sql_query'])\n",
    "    \n",
    "    ### split the query in tokens\n",
    "    token_list = sql_query_parsed[0].tokens\n",
    "    from_index = 0\n",
    "    \n",
    "    for index, token in enumerate(token_list):\n",
    "        #Token.Keyword.DML\n",
    "        ### identify if it is a select statement\n",
    "        if (token.is_keyword and str(token) == 'SELECT') is True:\n",
    "            ### set the select_index\n",
    "            select_index = index\n",
    "            break\n",
    "    for index, token in enumerate(token_list):\n",
    "        if (token.is_keyword and str(token) == 'FROM') is True:\n",
    "            from_index = index\n",
    "            break\n",
    "    token_list_with_counter_name = token_list[:]\n",
    "    token_list_with_counter_name.insert(from_index - 1, \" AS counter_value, TO_DATE(CURRENT_DATE) AS run_day  \")\n",
    "    token_list_with_counter_name.insert(select_index + 1, \" '\" + counter_row['counter'] + \"' AS counter_name, \")\n",
    "    enhanced_query_list = [str(token) for token in token_list_with_counter_name]\n",
    "    enhanced_query = ''.join(enhanced_query_list)\n",
    "    \n",
    "    return enhanced_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_join_to_namespaces(counter_row):\n",
    "    sql_query_parsed = sqlparse.parse(counter_row['sql_query'])\n",
    "    token_list_with_counter_name = sql_query_parsed[0].tokens\n",
    "    where_index = 0\n",
    "    select_index = 0\n",
    "    from_index = 0\n",
    "    for index, token in enumerate(token_list_with_counter_name):        #Token.Keyword.DML\n",
    "        if (token.is_keyword and str(token) == 'SELECT') is True:\n",
    "            select_index = index\n",
    "        if (token.is_keyword and str(token) == 'FROM') is True:\n",
    "            from_index = index\n",
    "        if isinstance(token, Where) is True:\n",
    "            where_index = index\n",
    "            break\n",
    "\n",
    "    new_tok_list = token_list_with_counter_name[:]\n",
    "    \n",
    "    join_to_insert = ''\n",
    "    if counter_row.table_name != 'users' and counter_row.table_name != 'namespaces':\n",
    "        join_to_insert = ' LEFT JOIN ' + counter_row.foreign_table_name + ' ON ' + counter_row.foreign_table_name + '.' + counter_row.foreign_column_name + ' = ' + counter_row.table_name + '.' + counter_row.column_name + ' '\n",
    "    if counter_row.foreign_table_name == 'projects'  and counter_row.table_name != 'namespaces':\n",
    "        join_to_insert += ' LEFT JOIN namespaces ON projects.namespace_id = namespaces.id'\n",
    "        \n",
    "    join_to_insert  = join_to_insert + \" LEFT JOIN {{ref('gitlab_dotcom_namespaces_xf')}} AS namespaces_xf ON namespaces.id = namespaces_xf.namespace_id \"\n",
    "    if where_index > 0:\n",
    "        token_list_with_counter_name.insert(where_index + 1 , ' GROUP BY 1')\n",
    "        token_list_with_counter_name.insert(where_index, join_to_insert)\n",
    "    else:\n",
    "        token_list_with_counter_name.append(join_to_insert)\n",
    "        token_list_with_counter_name.append(' GROUP BY 1')\n",
    "\n",
    "    token_list_with_counter_name.insert(from_index - 1, \" AS counter_value \")\n",
    "    if select_index >= 0:\n",
    "        token_list_with_counter_name.insert(select_index + 1, ' namespaces_xf.namespace_id, TO_DATE(CURRENT_DATE) AS run_day, ')\n",
    "\n",
    "    enhanced_query_list = [str(token) for token in token_list_with_counter_name]\n",
    "    enhanced_query = ''.join(enhanced_query_list)\n",
    "    \n",
    "    return enhanced_query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_sql_model_files(counter_row, sql_query_column, counter_name_column, subdirectory, suffix=''):\n",
    "    #subdirectory= 'models/workspaces/test_usage_pings/'\n",
    "    try:\n",
    "        os.makedirs(subdirectory)\n",
    "    except Exception:\n",
    "        pass\n",
    "    model_name = counter_row[counter_name_column].replace('.',  '_') + suffix + '.sql'\n",
    "    wr = open(os.path.join(subdirectory, model_name), 'w')\n",
    "    wr.write(counter_row[sql_query_column])\n",
    "    wr.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_query_tables(query):\n",
    "    '''\n",
    "    function to rename the table based on a new regex\n",
    "    '''\n",
    "    \n",
    "    ### comprehensive list of all the keywords that are followed by a table name\n",
    "    keyword_to_look_at = [            \n",
    "                'FROM',\n",
    "                \"JOIN\",\n",
    "                \"INNER JOIN\",\n",
    "                \"FULL JOIN\",\n",
    "                \"FULL OUTER JOIN\",\n",
    "                \"LEFT JOIN\",\n",
    "                \"RIGHT JOIN\",\n",
    "                \"LEFT OUTER JOIN\",\n",
    "                \"RIGHT OUTER JOIN\",\n",
    "    ]\n",
    "\n",
    "    \n",
    "    ### start parsing the query and get the token_list\n",
    "    parsed = sqlparse.parse(query)\n",
    "    tokens = list(TokenList(parsed[0].tokens).flatten())\n",
    "    ### setting up to -1 to start\n",
    "    keyword_token_index = -1\n",
    "    \n",
    "    while keyword_token_index != 0:\n",
    "        keyword_token_index = 0\n",
    "        \n",
    "        ### go through the tokens\n",
    "        for index, token in enumerate(tokens):\n",
    "            \n",
    "            if str(token) in keyword_to_look_at:\n",
    "                keyword_token_index = index\n",
    "                i = 1\n",
    "                while tokens[index + i].ttype is Whitespace:\n",
    "                    i += 1\n",
    "                next_token = tokens[index + i]\n",
    "                if str(next_token).startswith('prep') is False:\n",
    "                    tokens.insert(keyword_token_index + i, \"prep.gitlab_dotcom.gitlab_dotcom_\" + str(next_token) + \"_dedupe_source AS \" )\n",
    "                    tokens = [str(token) for token in tokens]\n",
    "                    token_query = ''.join(tokens)\n",
    "                    parsed = sqlparse.parse(token_query)\n",
    "                    tokens = list(TokenList(parsed[0].tokens).flatten())\n",
    "                    break\n",
    "                else:\n",
    "                    keyword_token_index = 0\n",
    "            if keyword_token_index > 0:\n",
    "                break\n",
    "    return token_query\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_last_join_to_namespace_xf(sql_query):\n",
    "    sql_query_parsed = sqlparse.parse(sql_query)\n",
    "    token_list_with_counter_name = sql_query_parsed[0].tokens\n",
    "    where_index = 0\n",
    "    select_index = 0\n",
    "    from_index = 0\n",
    "    for index, token in enumerate(token_list_with_counter_name):        #Token.Keyword.DML\n",
    "        if isinstance(token, Where) is True:\n",
    "            where_index = index\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dict = {}\n",
    "\n",
    "for index, row in fj_df.iterrows():\n",
    "    fj_df.loc[index,'global_counter_query'] = add_counter_name_as_column(row)\n",
    "    fj_df.loc[index,'counter_per_namespace_query'] = add_join_to_namespaces(row)\n",
    "    tables = sql_metadata.get_query_tables(fj_df.loc[index,'global_counter_query'])\n",
    "    are_all_tables_in_analytics = True\n",
    "    for table in tables:\n",
    "        file_name = 'gitlab_dotcom_{}_dedupe_source.sql'.format(table)\n",
    "        result = []\n",
    "        dirname = os.getcwd()\n",
    "        model_path = os.path.join(dirname, 'models')\n",
    "        for root, dirs, files in os.walk(model_path):\n",
    "            if file_name in files:\n",
    "                result.append(os.path.join(root, file_name))\n",
    "                are_all_tables_in_analytics = True\n",
    "                break\n",
    "            else:\n",
    "                are_all_tables_in_analytics = False\n",
    "        if are_all_tables_in_analytics == False:\n",
    "            break\n",
    "    fj_df.loc[index,'global_counter_query'] = rename_query_tables(row['global_counter_query'])\n",
    "    #fj_df.loc[index,'counter_per_namespace_query'] = rename_query_tables(row['counter_per_namespace_query'])\n",
    "\n",
    "    final_dict[fj_df.loc[index,'counter']] = fj_df.loc[index,'global_counter_query']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('query_json.json', 'w') as f:\n",
    "    json.dump(final_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in fj_df.iterrows():\n",
    "    if fj_df.loc[index,'are_all_tables_in_analytics'] == 'True':\n",
    "        write_sql_model_files(row, 'global_counter_query', 'counter', 'models/workspaces/test_usage_pings/')\n",
    "        write_sql_model_files(row, 'counter_per_namespace_query', 'counter', 'models/workspaces/workspace_usage_pings_namespaces/',suffix='_namespaces')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINISHED, BELOW IS SCRATCH PAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_to_dedup = ['merge_requests', 'issues']\n",
    "\n",
    "table_format = \"\"\"\n",
    "  SELECT *\n",
    "  FROM {{{{ source('gitlab_dotcom', '{}') }}}}\n",
    "  QUALIFY ROW_NUMBER() OVER (PARTITION BY id ORDER BY updated_at DESC) = 1\n",
    "\"\"\"\n",
    "for dedup in tables_to_dedup:\n",
    "    formatted = table_format.format(dedup)\n",
    "    subdirectory= 'models/sources/gitlab_dotcom/dedupe'\n",
    "    try:\n",
    "        os.makedirs(subdirectory)\n",
    "    except Exception:\n",
    "        pass\n",
    "    model_name = 'gitlab_dotcom_{}_dedupe_source.sql'.format(dedup)\n",
    "    wr = open(os.path.join(subdirectory, model_name), 'w')\n",
    "    wr.write(formatted)\n",
    "    wr.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tok_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-11013997cafa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtok_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;31m#Token.Keyword.DML\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_keyword\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'SELECT'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mselect_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWhere\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tok_list' is not defined"
     ]
    }
   ],
   "source": [
    "for index, token in enumerate(tok_list):\n",
    "    #Token.Keyword.DML\n",
    "    if (token.is_keyword and str(token) == 'SELECT') is True:\n",
    "        select_index = index\n",
    "    if isinstance(token, Where) is True:\n",
    "        where_index = index\n",
    "\n",
    "new_tok_list = tok_list[:]\n",
    "new_tok_list.insert(where_index + 1 , ' GROUP BY 1')\n",
    "new_tok_list.insert(where_index, ' LEFT JOIN projects ON services.id = projects.project_id ')\n",
    "new_tok_list.insert(select_index + 1, ' namespace_id,')\n",
    "\n",
    "tok_list_str = [str(token) for token in new_tok_list]\n",
    "query_stringed = ''.join(tok_list_str)\n",
    "query_parsed = sqlparse.parse(query_stringed)\n",
    "tok_list_transformed = query_parsed[0].tokens\n",
    "\n",
    "query_stringed\n",
    "test_formatting = sqlparse.format(query_stringed)\n",
    "test_formatting\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'List' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-85d1e4833d0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m def _update_table_names(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtables\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msqlparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_keyword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m ) -> List[str]:\n\u001b[1;32m      4\u001b[0m     \"\"\"\n\u001b[1;32m      5\u001b[0m     \u001b[0mReturn\u001b[0m \u001b[0mnew\u001b[0m \u001b[0mtable\u001b[0m \u001b[0mnames\u001b[0m \u001b[0mmatching\u001b[0m \u001b[0mdatabase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtable\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdatabase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtable\u001b[0m \u001b[0mnotation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'List' is not defined"
     ]
    }
   ],
   "source": [
    "def _update_table_names(\n",
    "    tables: List[str], tokens: List[sqlparse.sql.Token], index: int, last_keyword: str\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Return new table names matching database.table or database.schema.table notation\n",
    "    :type tables list[str]\n",
    "    :type tokens list[sqlparse.sql.Token]\n",
    "    :type index int\n",
    "    :type last_keyword str\n",
    "    :rtype: list[str]\n",
    "    \"\"\"\n",
    "\n",
    "    token = tokens[index]\n",
    "    last_token = tokens[index - 1].value.upper() if index > 0 else None\n",
    "    next_token = tokens[index + 1].value.upper() if index + 1 < len(tokens) else None\n",
    "\n",
    "    if (\n",
    "        last_keyword\n",
    "        in [\n",
    "            \"FROM\",\n",
    "            \"JOIN\",\n",
    "            \"INNER JOIN\",\n",
    "            \"FULL JOIN\",\n",
    "            \"FULL OUTER JOIN\",\n",
    "            \"LEFT JOIN\",\n",
    "            \"RIGHT JOIN\",\n",
    "            \"LEFT OUTER JOIN\",\n",
    "            \"RIGHT OUTER JOIN\",\n",
    "            \"INTO\",\n",
    "            \"UPDATE\",\n",
    "            \"TABLE\",\n",
    "        ]\n",
    "        and last_token not in [\"AS\"]\n",
    "        and token.value not in [\"AS\", \"SELECT\"]\n",
    "    ):\n",
    "        if last_token == \".\" and next_token != \".\":\n",
    "            # we have database.table notation example\n",
    "            table_name = \"{}.{}\".format(tokens[index - 2], tokens[index])\n",
    "            if len(tables) > 0:\n",
    "                tables[-1] = table_name\n",
    "            else:\n",
    "                tables.append(table_name)\n",
    "\n",
    "        schema_notation_match = (Name, \".\", Name, \".\", Name)\n",
    "        schema_notation_tokens = (\n",
    "            (\n",
    "                tokens[index - 4].ttype,\n",
    "                tokens[index - 3].value,\n",
    "                tokens[index - 2].ttype,\n",
    "                tokens[index - 1].value,\n",
    "                tokens[index].ttype,\n",
    "            )\n",
    "            if len(tokens) > 4\n",
    "            else None\n",
    "        )\n",
    "        if schema_notation_tokens == schema_notation_match:\n",
    "            # we have database.schema.table notation example\n",
    "            table_name = \"{}.{}.{}\".format(\n",
    "                tokens[index - 4], tokens[index - 2], tokens[index]\n",
    "            )\n",
    "            if len(tables) > 0:\n",
    "                tables[-1] = table_name\n",
    "            else:\n",
    "                tables.append(table_name)\n",
    "        elif tokens[index - 1].value.upper() not in [\",\", last_keyword]:\n",
    "            # it's not a list of tables, e.g. SELECT * FROM foo, bar\n",
    "            # hence, it can be the case of alias without AS, e.g. SELECT * FROM foo bar\n",
    "            pass\n",
    "        else:\n",
    "            table_name = str(token.value.strip(\"`\"))\n",
    "            tables.append(table_name)\n",
    "\n",
    "    return tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_test = 'SELECT test FROM \"services\" LEFT JOIN \"users\" ON users.id = services.id WHERE users.id = 5'\n",
    "\n",
    "sql_test = sql_test.replace('\"', \"\")\n",
    "#print(sql_test)\n",
    "parsed = sqlparse.parse(sql_test)\n",
    "tokens = TokenList(parsed[0].tokens).flatten()\n",
    "# print([(token.value, token.ttype) for token in tokens])\n",
    "\n",
    "test = [token for token in tokens if token.ttype is not Whitespace]\n",
    "\n",
    "table_syntax_keywords = [\n",
    "    # SELECT queries\n",
    "    \"FROM\",\n",
    "    \"WHERE\",\n",
    "    \"JOIN\",\n",
    "    \"INNER JOIN\",\n",
    "    \"FULL JOIN\",\n",
    "    \"FULL OUTER JOIN\",\n",
    "    \"LEFT OUTER JOIN\",\n",
    "    \"RIGHT OUTER JOIN\",\n",
    "    \"LEFT JOIN\",\n",
    "    \"RIGHT JOIN\",\n",
    "    \"ON\",\n",
    "    # INSERT queries\n",
    "    \"INTO\",\n",
    "    \"VALUES\",\n",
    "    # UPDATE queries\n",
    "    \"UPDATE\",\n",
    "    \"SET\",\n",
    "    # Hive queries\n",
    "    \"TABLE\",  # INSERT TABLE\n",
    "]\n",
    "\n",
    "tables = []\n",
    "last_keyword = None\n",
    "last_token=None\n",
    "\n",
    "print(test)\n",
    "for index, token in enumerate(test):\n",
    "    print(token.ttype is Name)\n",
    "    #print([token, token.ttype, last_token, last_keyword, token.is_keyword, index])\n",
    "    if token.is_keyword and token.value.upper() == 'WHERE':\n",
    "        # keep the name of the last keyword, the next one can be a table name\n",
    "        where_index = index\n",
    "        print(where_index)\n",
    "    elif (\n",
    "        token.is_keyword\n",
    "        and str(token).upper() == \"SELECT\"\n",
    "    ):\n",
    "        # reset the last_keyword for \"INSERT INTO SELECT\" and \"INSERT TABLE SELECT\" queries\n",
    "        last_keyword = None\n",
    "        select_index = index\n",
    "        print(2)\n",
    "    elif token.ttype is Name or token.is_keyword:\n",
    "        tables.append(str(token))\n",
    "        print(3)\n",
    "        \n",
    "print(tables)\n",
    "tables[0] = 'replace'\n",
    "tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in final_join.iterrows():\n",
    "   print(row['table_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in final_join_mapping_df.iterrows():\n",
    "    if row.table_name != 'users':\n",
    "        join_to_insert = 'LEFT JOIN ' + row.foreign_table_name + ' ON ' + row.foreign_table_name + '.' + row.foreign_column_name + ' = ' + row.table_name + '.' + row.column_name\n",
    "        if row.foreign_table_name == 'projects':\n",
    "            join_to_insert += ' LEFT JOIN namespaces ON projects.namespace_id = namespaces.id '\n",
    "        print(join_to_insert)\n",
    "    parsed_sql_query = sqlparse.parse(row.sql_query)[0]\n",
    "    parsed_join_to_insert = sqlparse.parse(join_to_insert)\n",
    "    parsed_sql_query.insert_before(-1, parsed_join_to_insert[0])\n",
    "    print(parsed_sql_query)\n",
    "    print(row.counter)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## verify it is a where clause\n",
    "\n",
    "parsed = sqlparse.parse(value)\n",
    "where = parsed[0][-1]\n",
    "print(where)\n",
    "where_clause = False\n",
    "for i in where.tokens:\n",
    "    if str(i).find('WHERE') >= 0:\n",
    "        where_clause = True\n",
    "    \n",
    "for i in parsed[0].tokens:\n",
    "    try:\n",
    "        for j in i.tokens:\n",
    "\n",
    "            if str(j) == '\"services\"':\n",
    "                print(j)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "idx, _ = parsed[0].token_next_by(m=(Identifier, 'WHERE'))\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import sqlparse\n",
    "from sqlparse.sql import Identifier, IdentifierList, remove_quotes, Token, TokenList, Where\n",
    "from sqlparse.tokens import Keyword, Name, Punctuation, String, Whitespace\n",
    "from sqlparse.utils import imt\n",
    "import pandas as pd\n",
    "\n",
    "sql_first = sql_queries_dict[\"counts.issues\"]\n",
    "\n",
    "parsed = sqlparse.parse(sql_first)[0]\n",
    "test_list = []\n",
    "\n",
    "print(parsed.get_name())\n",
    "\n",
    "parsed._pprint_tree()\n",
    "where_statment = parsed[-1]\n",
    "select_statement = parsed\n",
    "\n",
    "for x in select_statement:\n",
    "    test_list.append(str(x))\n",
    "    \n",
    "test_list\n",
    "\n",
    "left_join = ' LEFT JOIN projects ON test.project_id = projects.id'\n",
    "test_list.append(left_join)\n",
    "test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "table_to_look = 'issues'\n",
    "table_to_join = ['projects', 'namespaces', 'groups']\n",
    "foreign_key_df.head(20)\n",
    "\n",
    "potential_joins = foreign_key_df[(foreign_key_df['table_name'] == table_to_look) & (foreign_key_df['foreign_table_name'].isin(table_to_join))]\n",
    "\n",
    "for index, row in final_join.iterrows():\n",
    "    if row.table_name != 'users':\n",
    "        print('LEFT JOIN ' + row.foreign_table_name + ' ON ' + row.foreign_table_name + '.' + row.foreign_column_name + ' = ' + row.table_name + '.' + row.column_name)\n",
    "        if row.foreign_table_name == 'projects':\n",
    "            print('LEFT JOIN namespaces ON projects.namespace_id = namespaces.id')\n",
    "    print(row.table_name)\n",
    "    print(row.counter)\n",
    "\n",
    "    \n",
    "group_by_statement = 'GROUP BY 1'\n",
    "print(group_by_statement)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isinstance(where_statment, Where)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx, _ = parsed.token_next_by(m=(Keyword, \"BETWEEN\"))\n",
    "print(idx)\n",
    "if idx is not None:\n",
    "    _, token = parsed.token_next(idx=idx)\n",
    "    if token:\n",
    "        if isinstance(token, IdentifierList):\n",
    "            # In case of \"LIMIT <offset>, <limit>\", find comma and extract\n",
    "            # first succeeding non-whitespace token\n",
    "            idx, _ = token.token_next_by(m=(sqlparse.tokens.Punctuation, \",\"))\n",
    "            _, token = token.token_next(idx=idx)\n",
    "        if token and token.ttype == sqlparse.tokens.Literal.Number.Integer:\n",
    "            print(int(token.value))\n",
    "\n",
    "where = next(token for token in parsed.tokens if isinstance(token, Where))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = 'SELECT test FROM \"services\" LEFT JOIN \"users\" ON users.id = services.id WHERE users.id = 5'\n",
    "\n",
    "elements = sqlparse.parse(value)\n",
    "tok_list = elements[0].tokens\n",
    "\n",
    "isinstance(tok_list[-1], Where)\n",
    "\n",
    "tok_list\n",
    "\n",
    "## add group by at the end\n",
    "## add joins (what if there is a namespace, group or project table already)\n",
    "## add namespace_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, token in enumerate(tok_list):\n",
    "    #Token.Keyword.DML\n",
    "    if (token.is_keyword and str(token) == 'SELECT') is True:\n",
    "        select_index = index\n",
    "    if isinstance(token, Where) is True:\n",
    "        where_index = index\n",
    "\n",
    "new_tok_list = tok_list[:]\n",
    "new_tok_list.insert(where_index + 1 , ' GROUP BY 1')\n",
    "new_tok_list.insert(where_index, ' LEFT JOIN projects ON services.id = projects.project_id ')\n",
    "new_tok_list.insert(select_index + 1, ' namespace_id,')\n",
    "\n",
    "tok_list_str = [str(token) for token in new_tok_list]\n",
    "query_stringed = ''.join(tok_list_str)\n",
    "query_parsed = sqlparse.parse(query_stringed)\n",
    "tok_list_transformed = query_parsed[0].tokens\n",
    "\n",
    "query_stringed\n",
    "test_formatting = sqlparse.format(query_stringed)\n",
    "test_formatting\n",
    "\n",
    "wr = open('test.sql', 'w')\n",
    "wr.write(test_formatting)\n",
    "wr.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
